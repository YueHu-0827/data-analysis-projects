{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07b7ab67",
      "metadata": {
        "id": "07b7ab67"
      },
      "source": [
        "# GR5398 26Spring: FinGPT Large Language Model Track\n",
        "## Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6db642b",
      "metadata": {
        "id": "b6db642b"
      },
      "source": [
        "### 1. Data Preparation\n",
        "\n",
        "In this part, you can build your own dataset for later training. Here is the example of how we generate dataset from Dow Jones 30's component stocks.\n",
        "\n",
        "We used **Finnhub** to get raw data, and **GPT-4** to generate benchmark responses (you can change these to get a better result, maybe).\n",
        "\n",
        "For detailed information of this part, please refer to [`prepare_data.ipynb`](https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_Forecaster/prepare_data.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e305c2e7",
      "metadata": {
        "id": "e305c2e7"
      },
      "outputs": [],
      "source": [
        "!pip install finnhub-python\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import finnhub\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from datasets import Dataset\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5108fee6",
      "metadata": {
        "id": "5108fee6"
      },
      "source": [
        "+ Raw Financial Data Acquisition (News and Returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab07882e",
      "metadata": {
        "id": "ab07882e"
      },
      "outputs": [],
      "source": [
        "# Load Dow Jones 30 dataset provided by FinGPT (May 2023 - May 2024)\n",
        "splits = {\n",
        "    \"train\": \"data/train-00000-of-00001-7c4c80aa07272d4c.parquet\",\n",
        "    \"test\": \"data/test-00000-of-00001-28531804b005ddc6.parquet\"\n",
        "}\n",
        "\n",
        "train_df = pd.read_parquet(\n",
        "    \"hf://datasets/FinGPT/fingpt-forecaster-dow30-202305-202405/\"\n",
        "    + splits[\"train\"]\n",
        ")\n",
        "\n",
        "test_df = pd.read_parquet(\n",
        "    \"hf://datasets/FinGPT/fingpt-forecaster-dow30-202305-202405/\"\n",
        "    + splits[\"test\"]\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Test size:\", len(test_df))\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ec77e0c",
      "metadata": {
        "id": "3ec77e0c"
      },
      "source": [
        "+ Transform to Llama Training Format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "716a4861",
      "metadata": {
        "id": "716a4861"
      },
      "source": [
        "+ Test-time Information Fetching"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3fe8adc",
      "metadata": {
        "id": "a3fe8adc"
      },
      "source": [
        "### 2. Fine-tune LLM\n",
        "\n",
        "This is the core part of fine-tuning, which needs **DeepSpeed** to help you manage your VRAM while training on GPU(s). Since you need a brand new subprocess to launch DeepSpeed, here we don't provide you with the code for fine-tuning. Instead, we highly suggest you to run `train.sh` on your own terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba5c98e",
      "metadata": {
        "id": "5ba5c98e"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import textwrap\n",
        "\n",
        "### Adjust based on your setup\n",
        "cmd = textwrap.dedent(\"\"\"\n",
        "export NCCL_IGNORE_DISABLED_P2P=1\n",
        "export TRANSFORMERS_NO_ADVISORY_WARNINGS=1\n",
        "export TOKENIZERS_PARALLELISM=0\n",
        "\n",
        "ds \\\n",
        "    --include localhost:0 \\\n",
        "    train_lora.py \\\n",
        "    --run_name nasdaq-100-20231231-20241231 \\\n",
        "    --base_model llama2 \\\n",
        "    --dataset fingpt-forecaster-nasdaq-100-20231231-20241231-1-4-06 \\\n",
        "    --max_length 4096 \\\n",
        "    --batch_size 1 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --num_epochs 1 \\\n",
        "    --log_interval 10 \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --scheduler constant \\\n",
        "    --evaluation_strategy steps \\\n",
        "    --ds_config config.json\n",
        "\"\"\")\n",
        "\n",
        "subprocess.run(cmd, shell=True, executable=\"/bin/bash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b9e630f",
      "metadata": {
        "id": "4b9e630f"
      },
      "source": [
        "### 3. Have a try on your own fine-tuned LLMs\n",
        "\n",
        "In this part, you can have a try on your fine-tuned models by providing it with some inputs and see their responses.\n",
        "\n",
        "If you made it, congratulations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947d3e40",
      "metadata": {
        "id": "947d3e40"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f9bcc86",
      "metadata": {
        "id": "9f9bcc86"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cache_dir = \"./pretrained-models\" ### Adjust based on your setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2c8808",
      "metadata": {
        "id": "ac2c8808"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Ensure your Hugging Face token is set as a Colab secret named 'HF_TOKEN'\n",
        "# Or, uncomment the line below and replace 'hf_token_here' with your actual token string\n",
        "# login(token='hf_token_here')\n",
        "login() # This will use the HF_TOKEN from Colab secrets if available\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.1-8B\", ### Change to your base model, e.g., 'meta-llama/Llama-2-7b-hf' or a local path\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    cache_dir=cache_dir,\n",
        "    torch_dtype=torch.float16,   # optional if you have enough VRAM\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'meta-llama/Llama-3.1-8B', ### Change to your base model\n",
        "    cache_dir=cache_dir,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    'your_finetuned_model_path_here', ### Change to your fine-tuned model path or Hugging Face identifier\n",
        "    cache_dir=cache_dir,\n",
        "    # offload_folder=\"./offload2/\",\n",
        "    torch_dtype=torch.float16,\n",
        "    # offload_buffers=True\n",
        ")\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295a72fa",
      "metadata": {
        "id": "295a72fa"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    Your prompt here\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b4a607",
      "metadata": {
        "id": "94b4a607"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors='pt',\n",
        "    max_length=4096,\n",
        "    padding=False,\n",
        "    truncation=True\n",
        ")\n",
        "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "\n",
        "res = model.generate(\n",
        "    **inputs, max_length=4096, do_sample=True,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=True\n",
        ")\n",
        "output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
        "answer = re.sub(r'.*\\[/INST\\]\\s*', '', output, flags=re.DOTALL) # don't forget to import re\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b80e700",
      "metadata": {
        "id": "5b80e700"
      },
      "source": [
        "### 4. Comparison\n",
        "\n",
        "Now since you get both fine-tuned LLMs based on DeepSeek, Llama3 and Qwen, here we would like you to do some comparison on selected metrics, to see which of these 2 fine-tuned models performs best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "670693a3",
      "metadata": {
        "id": "670693a3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from datasets import load_from_disk\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "# from peft import PeftModel\n",
        "from utils import *\n",
        "import time\n",
        "import json, pickle\n",
        "\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = \"your huggingface token\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec127051",
      "metadata": {
        "id": "ec127051"
      },
      "outputs": [],
      "source": [
        "# Llama3\n",
        "llama3_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    'meta-llama/Llama-3.1-8B',\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    cache_dir=cache_dir,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "llama3_model = PeftModel.from_pretrained(\n",
        "    llama3_base_model,\n",
        "    'your_finetuned_model', ### Change to your fine-tuned model\n",
        "    cache_dir=cache_dir,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "llama3_model = llama3_model.eval()\n",
        "\n",
        "llama3_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'meta-llama/Llama-3.1-8B',\n",
        "    cache_dir=cache_dir,\n",
        ")\n",
        "llama3_tokenizer.padding_side = \"right\"\n",
        "llama3_tokenizer.pad_token_id = llama3_tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ae5b26",
      "metadata": {
        "id": "e8ae5b26"
      },
      "outputs": [],
      "source": [
        "# DeepSeek\n",
        "deepseek_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    cache_dir=cache_dir,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "deepseek_model = PeftModel.from_pretrained(\n",
        "    deepseek_base_model,\n",
        "    'your_finetuned_model', ### Change to your fine-tuned model\n",
        "    cache_dir=cache_dir,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "deepseek_model = deepseek_model.eval()\n",
        "\n",
        "deepseek_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
        "    cache_dir=cache_dir,\n",
        ")\n",
        "deepseek_tokenizer.padding_side = \"right\"\n",
        "deepseek_tokenizer.pad_token_id = deepseek_tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30501d01",
      "metadata": {
        "id": "30501d01"
      },
      "outputs": [],
      "source": [
        "test_dataset = load_dataset(\"your_dataset_name\")[0][\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e5f36d",
      "metadata": {
        "id": "26e5f36d"
      },
      "outputs": [],
      "source": [
        "def filter_by_ticker(test_dataset, ticker_code):\n",
        "\n",
        "    filtered_data = []\n",
        "\n",
        "    for row in test_dataset:\n",
        "        prompt_content = row['prompt']\n",
        "\n",
        "        ticker_symbol = re.search(r\"ticker\\s([A-Z]+)\", prompt_content)\n",
        "\n",
        "        if ticker_symbol and ticker_symbol.group(1) == ticker_code:\n",
        "            filtered_data.append(row)\n",
        "\n",
        "    filtered_dataset = Dataset.from_dict({key: [row[key] for row in filtered_data] for key in test_dataset.column_names})\n",
        "\n",
        "    return filtered_dataset\n",
        "\n",
        "def get_unique_ticker_symbols(test_dataset):\n",
        "\n",
        "    ticker_symbols = set()\n",
        "\n",
        "    for i in range(len(test_dataset)):\n",
        "        prompt_content = test_dataset[i]['prompt']\n",
        "\n",
        "        ticker_symbol = re.search(r\"ticker\\s([A-Z]+)\", prompt_content)\n",
        "\n",
        "        if ticker_symbol:\n",
        "            ticker_symbols.add(ticker_symbol.group(1))\n",
        "\n",
        "    return list(ticker_symbols)\n",
        "\n",
        "def insert_guidance_after_intro(prompt):\n",
        "\n",
        "    intro_marker = (\n",
        "        \"[INST]<<SYS>>\\n\"\n",
        "        \"You are a seasoned stock market analyst. Your task is to list the positive developments and \"\n",
        "        \"potential concerns for companies based on relevant news and basic financials from the past weeks, \"\n",
        "        \"then provide an analysis and prediction for the companies' stock price movement for the upcoming week.\"\n",
        "    )\n",
        "    guidance_start_marker = \"Based on all the information before\"\n",
        "    guidance_end_marker = \"Following these instructions, please come up with 2-4 most important positive factors\"\n",
        "\n",
        "    intro_pos = prompt.find(intro_marker)\n",
        "    guidance_start_pos = prompt.find(guidance_start_marker)\n",
        "    guidance_end_pos = prompt.find(guidance_end_marker)\n",
        "\n",
        "    if intro_pos == -1 or guidance_start_pos == -1 or guidance_end_pos == -1:\n",
        "        return prompt\n",
        "\n",
        "    guidance_section = prompt[guidance_start_pos:guidance_end_pos].strip()\n",
        "\n",
        "    new_prompt = (\n",
        "        f\"{prompt[:intro_pos + len(intro_marker)]}\\n\\n\"\n",
        "        f\"{guidance_section}\\n\\n\"\n",
        "        f\"{prompt[intro_pos + len(intro_marker):guidance_start_pos]}\"\n",
        "        f\"{prompt[guidance_end_pos:]}\"\n",
        "    )\n",
        "\n",
        "    return new_prompt\n",
        "\n",
        "\n",
        "def apply_to_all_prompts_in_dataset(test_dataset):\n",
        "\n",
        "    updated_dataset = test_dataset.map(lambda x: {\"prompt\": insert_guidance_after_intro(x[\"prompt\"])})\n",
        "\n",
        "    return updated_dataset\n",
        "\n",
        "test_dataset = apply_to_all_prompts_in_dataset(test_dataset)\n",
        "\n",
        "unique_symbols = set(test_dataset['symbol'])\n",
        "\n",
        "def test_demo(model, tokenizer, prompt):\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt, return_tensors='pt',\n",
        "        padding=False, max_length=8000\n",
        "    )\n",
        "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "\n",
        "    start_time = time.time()\n",
        "    res = model.generate(\n",
        "        **inputs, max_length=4096, do_sample=True,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=True\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
        "    return output, end_time - start_time\n",
        "\n",
        "def test_acc(test_dataset, modelname):\n",
        "    answers_base, answers_fine_tuned, gts, times_base, times_fine_tuned = [], [], [], [], []\n",
        "    if modelname == \"llama3\":\n",
        "        base_model = llama3_base_model\n",
        "        model = llama3_model\n",
        "        tokenizer = llama3_tokenizer\n",
        "    elif modelname == \"deepseek\":\n",
        "        base_model = deepseek_base_model\n",
        "        model = deepseek_model\n",
        "        tokenizer = deepseek_tokenizer\n",
        "    ### Add other models here\n",
        "    elif modelname == \"your_model_name\":  # Add other models as needed\n",
        "        base_model = your_base_model  # Define your base model\n",
        "        model = your_finetuned_model  # Define your fine-tuned model\n",
        "        tokenizer = your_tokenizer     # Define your tokenizer\n",
        "\n",
        "    for i in tqdm(range(len(test_dataset)), desc=\"Processing test samples\"):\n",
        "        try:\n",
        "            prompt = test_dataset[i]['prompt']\n",
        "            gt = test_dataset[i]['answer']\n",
        "\n",
        "            output_base, time_base = test_demo(base_model, tokenizer, prompt)\n",
        "            answer_base = re.sub(r'.*\\[/INST\\]\\s*', '', output_base, flags=re.DOTALL)\n",
        "\n",
        "            output_fine_tuned, time_fine_tuned = test_demo(model, tokenizer, prompt)\n",
        "            answer_fine_tuned = re.sub(r'.*\\[/INST\\]\\s*', '', output_fine_tuned, flags=re.DOTALL)\n",
        "\n",
        "            answers_base.append(answer_base)\n",
        "            answers_fine_tuned.append(answer_fine_tuned)\n",
        "            gts.append(gt)\n",
        "            times_base.append(time_base)\n",
        "            times_fine_tuned.append(time_fine_tuned)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i}: {e}\")\n",
        "    return answers_base, answers_fine_tuned, gts, times_base, times_fine_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87608f38",
      "metadata": {
        "id": "87608f38"
      },
      "outputs": [],
      "source": [
        "### Llama3 Result Evaluating\n",
        "\n",
        "llama3_answers_base, llama3_answers_fine_tuned, llama3_gts, llama3_base_times, llama3_fine_tuned_times = test_acc(test_dataset, \"llama3\")\n",
        "llama3_base_metrics = calc_metrics(llama3_answers_base, llama3_gts)\n",
        "llama3_fine_tuned_metrics = calc_metrics(llama3_answers_fine_tuned, llama3_gts)\n",
        "\n",
        "with open(\"./comparison_results/llama3_base_metrics.pkl\", \"wb\") as f:\n",
        "    pickle.dump(llama3_base_metrics, f)\n",
        "\n",
        "with open(\"./comparison_results/llama3_fine_tuned_metrics.pkl\", \"wb\") as f:\n",
        "    pickle.dump(llama3_fine_tuned_metrics, f)\n",
        "\n",
        "with open(\"./comparison_results/llama3_base_times.pkl\", \"wb\") as f:\n",
        "    pickle.dump(llama3_base_times, f)\n",
        "\n",
        "with open(\"./comparison_results/llama3_fine_tuned_times.pkl\", \"wb\") as f:\n",
        "    pickle.dump(llama3_fine_tuned_times, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b5df00",
      "metadata": {
        "id": "82b5df00"
      },
      "outputs": [],
      "source": [
        "### DeepSeek Result Evaluating\n",
        "\n",
        "deepseek_answers_base, deepseek_answers_fine_tuned, deepseek_gts, deepseek_base_times, deepseek_fine_tuned_times = test_acc(test_dataset, \"deepseek\")\n",
        "deepseek_base_metrics = calc_metrics(deepseek_answers_base, deepseek_gts)\n",
        "deepseek_fine_tuned_metrics = calc_metrics(deepseek_answers_fine_tuned, deepseek_gts)\n",
        "\n",
        "with open(\"./comparison_results/deepseek_base_metrics.pkl\", \"wb\") as f:\n",
        "    pickle.dump(deepseek_base_metrics, f)\n",
        "\n",
        "with open(\"./comparison_results/deepseek_fine_tuned_metrics.pkl\", \"wb\") as f:\n",
        "    pickle.dump(deepseek_fine_tuned_metrics, f)\n",
        "\n",
        "with open(\"./comparison_results/deepseek_base_times.pkl\", \"wb\") as f:\n",
        "    pickle.dump(deepseek_base_times, f)\n",
        "\n",
        "with open(\"./comparison_results/deepseek_fine_tuned_times.pkl\", \"wb\") as f:\n",
        "    pickle.dump(deepseek_fine_tuned_times, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6667128f",
      "metadata": {
        "id": "6667128f"
      },
      "outputs": [],
      "source": [
        "### Comparing Llama3 and DeepSeek Results\n",
        "\n",
        "comparison_matrics = calc_metrics(llama3_answers_fine_tuned, deepseek_answers_fine_tuned) ### Change based on your models\n",
        "\n",
        "with open(\"./comparison_results/comparison_matrics.pkl\", \"wb\") as f:\n",
        "    pickle.dump(comparison_matrics, f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "FinRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}